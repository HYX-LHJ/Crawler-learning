# Day03 —— 文档型网页结构解析与多页面爬虫（学习总结）

Day03 的目标是从「能访问网页」升级为「能解析网页结构、提取正文并保存内容」，这是爬虫从入门走向工程应用的关键一步。

## 🎯 今日掌握的核心能力

### 1. 理解文档类网页的 HTML 结构

文档类网站（如 ReadTheDocs、Sphinx、博客、Wiki）的正文结构具有规律性：

- `<h1>`：页面主标题
- `<h2>`：一级小标题（章节名称）
- `<h3>`：二级小标题（子章节名称）
- `<p>`：正文文本段落
- `.section`：正文内容所在的容器（Sphinx 文档的标准结构）

这些标签的含义和层级，对后续解析结构化内容至关重要。

### 2. 正确选择 CSS Selector 提取网页内容

你掌握了多个关键选择器：

| 目标 | Selector |
|------|----------|
| 页面主标题 | `h1` |
| 正文段落 | `.section p` |
| 一级目录链接 | `.toctree-l1 a` |
| 文档中所有小节标题 | `.section h2, .section h3` |

你也理解了为什么不能直接用 `h2` 或 `h3`：它们可能包含侧边栏或导航栏的内容，而 `.section` 能确保只获取正文部分。

### 3. 多页面爬取的必要步骤

你成功实现了文档爬虫的标准流程：

1. 访问目录页
2. 提取所有文档链接（相对路径）
3. 使用 `urljoin` 拼接成绝对 URL
4. 遍历链接，依次爬取每个页面内容
5. 对每个页面：
   - 解析标题（h1）
   - 提取正文段落（p）
   - 清洗标题作为文件名
   - 保存为 txt 文件


这已经构成一个完整的小爬虫项目。

### 4. 相对路径 → 绝对路径

你掌握了生产级 URL 处理方法：

```python
from urllib.parse import urljoin
full_url = urljoin(base, href)
```

相较手工拼接，这种方式更稳健，可自动处理：

- `/path`
- `../path`
- 绝对路径
- 缺失 `/` 的路径

这是专业爬虫工程师必备技能。

### 5. 文件名清洗（避免文件系统报错）

标题中可能出现：`/ \ : * ? " < > |`

这些字符无法作为文件名，因此你实现了安全文件名函数：

```python
re.sub(r'[\/\\\:\*\?\"\<\>\|]', '_', title)
```


这是保存内容时不可或缺的一步。

### 6. 单页面解析 → 多页面解析的能力跃迁

你成功做到：

- 对单独页面正确解析标题 + 正文
- 扩展为对所有文档链接循环处理
- 保存为多个文件（如 `Introduction.txt`）

这标志着你从“学习基础工具”进入“能够写完整爬虫脚本”的阶段。

## 🧠 今日掌握的概念与思维方式

- 文档结构是有层次的，正确选择 selector 比盲目爬取更重要
- `.section` 是正文区域，这是文档类网站的关键定位点
- `urljoin` 是绝对可靠的 URL 拼接方式
- 文件名必须清洗，否则实际工程中会频繁报错
- 多页面爬虫的核心是"目录 → 递归访问 → 解析内容"
- 理解 HTML 结构比背代码更重要

这些都是爬虫工程的核心思维。

## 🔚 Day03 完结：你现在具备的能力

你已经能够：

- 提取目录结构
- 访问若干文档子页面
- 提取标题与正文
- 将内容保存成本地文件
- 组合多个技术点形成一个完整爬虫脚本

未来你完全可以在此基础上扩展：

- 全站爬取
- 递归爬取子目录
- 提取代码块、列表、表格等复杂结构
- 构建 Markdown 文档
- 结构化 JSON 存储
- 增加反爬策略